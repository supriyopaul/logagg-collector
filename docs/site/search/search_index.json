{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"logagg-fs Fuse file system for logagg A fuse file-system wich has to be mounted in the file path like '/var/log' so that logagg-collector can collect logs from the files in the file-system. This guarentees no logs are missed from the files even when the log-files are rotated. logagg-collector Log collector for logagg Collects all the logs from the server and parses it for making a common schema for all the logs and sends to NSQ.","title":"Overview"},{"location":"#logagg-fs","text":"Fuse file system for logagg A fuse file-system wich has to be mounted in the file path like '/var/log' so that logagg-collector can collect logs from the files in the file-system. This guarentees no logs are missed from the files even when the log-files are rotated.","title":"logagg-fs"},{"location":"#logagg-collector","text":"Log collector for logagg Collects all the logs from the server and parses it for making a common schema for all the logs and sends to NSQ.","title":"logagg-collector"},{"location":"collector/","text":"logagg-collector Log collector for logagg Track and collects all the logs from given files and parses them to make a common schema for all the logs and sends to NSQ. Prerequisites Python >= 3.5 We expect users to follow Best practices for logging their application. Most importantly, do structured logging. Since, parsing/formatting logs is way easier that way. Install, set-up and run logagg-fs beforehand. Components/Architecture/Terminology files : Log files which are being tracked by logagg node : The server(s) where the log files reside formatters : The parser function that the collector uses to format the log lines to put it the common format. nsq : The central location where logs are sent by collector (s) after formatting as messages. Features Guaranteed delivery of each log line from files. Reduced latency between a log being generated an being present in the nsq . Options to add custom formatters . File poll if log file not yet generated. Works on rotational log files. Custom formatters to support parsing of any log file. Output format of processed log lines (dictionary) id (str) - A unique id per log with time ordering. Useful to avoid storing duplicates. timestamp (str) - ISO Format time. eg: data (dict) - Parsed log data raw (str) - Raw log line read from the log file host (str) - Hostname of the node where this log was generated formatter (str) - name of the formatter that processed the raw log line file (str) - Full path of the file in the host where this log line came from type (str) - One of \"log\", \"metric\" (Is there one more type?) level (str) - Log level of the log line. event (str) - LOG event error (bool) - True if collection handler failed during processing error_tb (str) - Error traceback Installation Setup Install and run logagg-fs on the machine from where you need to collect the logs from. Install and run logagg-master service on the machine where you want to aggregate the logs. Install the Docker package, at both collector nodes. Run the following commands to install : $ sudo apt-get update $ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - $ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" $ sudo apt-get update $ sudo apt-get install docker-ce Check Docker version >= 17.12.1 $ sudo docker -v Docker version 18.03.1-ce, build 9ee9f40 Run serverstats to store server metrics in /var/log/serverstats $ docker plugin install deepcompute/docker-file-log-driver:1.0 --alias file-log-driver $ docker run --hostname $HOSTNAME --name serverstats --restart unless-stopped --label formatter=logag_gcollector.formatters.basescript --log-driver file-log-driver --log-opt labels=formatter --log-opt fpath=/serverstats/serverstats.log --detach deepcompute/serverstats:latest Install the logagg-collector package, at where we collect the logs: Run the following command to pip install logagg : $ pip3 install https://github.com/deep-compute/pygtail/tarball/master/#egg=pygtail-0.6.1 $ pip3 install git+https://github.com/deep-compute/logagg-utils.git $ pip3 install git+https://github.com/deep-compute/logagg-collector.git Basic Usage Run logagg-collector service $ sudo logagg-collector runserver --host <DNS/IP> --port <port_number> --master host=<master_host>:port=<master_port>:topic_name=<topic_name> --data-dir <path_to_data_dir> --logaggfs-dir <logagg-fs_logcache_directory> Command breakdown: --host <DNS/IP>: DNS/IP on which collector should run so that it is accesible to other components via API calls. --port <port_number>: Port number to run logagg-collector on. --master host=<master_host>:port=<master_port>:topic_name=<topic_name>: Master host, port to contact master and topic name in which messages are sent. To aggregate logs in one place, Run all collectors under same topic name. --data-dir <path_to_data_dir>: Directory where logagg-collector will store it's data. If running multiple collectors, specify different directories. --logaggfs-dir <logagg-fs_logcache_directory>: logagg-fs logcache directory path. APIs To see files collector is supposed to collect curl 'http://localhost:1099/collector/v1/get_files' To add files so that collector can poll/track them curl 'http://localhost:1099/collector/v1/add_file?fpath=\"/var/log/serverstats.log\"&formatter=\"logagg_collector.formatters.docker_file_log_driver\"' Full list of APIs here Types of handlers we support Formatter-name Comments logagg_collector.formatters.nginx_access See Configuration here logagg_collector.formatters.mongodb logagg_collector.formatters.basescript logagg_collector.formatters.docker_log_file_driver See example here Advanced usage Run collector without a master Install NSQ package to pool logs sent by collectors $ sudo apt-get install libsnappy-dev $ wget https://s3.amazonaws.com/bitly-downloads/nsq/nsq-1.0.0-compat.linux-amd64.go1.8.tar.gz $ tar zxvf nsq-1.0.0-compat.linux-amd64.go1.8.tar.gz $ sudo cp nsq-1.0.0-compat.linux-amd64.go1.8/bin/* /usr/local/bin Bring up the nsq instances at the required server with following commands: NOTE: Run each command in a seperate Terminal window nsqlookupd $ nsqlookupd nsqd -lookupd-tcp-address :4160 $ nsqd -lookupd-tcp-address localhost:4160 nsqadmin -lookupd-http-address :4161 $ nsqadmin -lookupd-http-address localhost:4161 Run logagg-collector service $ logagg-collector runserver --data-dir <path_to_data_dir> --logaggfs-dir <logagg-fs_logcache_directory> Add NSQ details manually via API command. $ curl 'http://localhost:1099/logagg/v1/set_nsq?nsqd_http_address=localhost:4151&topic_name=logagg' Start collecting $ curl 'http://localhost:1099/logagg/v1/start How to create and use custom formatters for log files Step 1: make a directory and append it's path to evironment variable $PYTHONPATH $ echo $PYTHONPATH $ mkdir customformatters $ #Now append the path to $PYTHONPATH $ export PYTHONPATH=$PYTHONPATH:/home/path/to/customformatters/ $ echo $PYTHONPATH :/home/path/to/customformatters Step 2: Create a another directory and put your formatter file(s) inside it. $ cd customformatters/ $ mkdir myformatters $ cd myformatters/ $ touch formatters.py $ touch __init__.py $ echo 'import formatters' >> __init__.py $ #Now write your formatter functions inside the formatters.py file Step 3: Write your formatter functions inside the formatters.py file Important: 1. Only python standard modules can be imported in formatters.py file 2. A formatter function should return a dict() datatype 3. The 'dict()' should only contain keys which are mentioned in the above log structure . 4. Sample formatter functions: import json import re sample_log_line = '2018-02-07T06:37:00.297610Z [Some_event] [Info] [Hello_there]' def sample_formatter(log_line): log = re.sub('[\\[+\\]]', '',log_line).split(' ') timestamp = log[0] event = log[1] level = log[2] data = dict({'message': log[3]}) return dict(timestamp = timestamp, event = event, level = level, data = data, ) To see more examples, look here Check if the custom handler works in python interpreter . >>> import myformatters >>> sample_log_line = '2018-02-07T06:37:00.297610Z [Some_event] [Info] [Hello_there]' >>> output = myformatters.formatters.sample_formatter(sample_log_line) >>> from pprint import pprint >>> pprint(output) {'data': {'message': 'Hello_there'}, 'event': 'Some_event', 'level': 'Info', 'timestamp': '2018-02-07T06:37:00.297610Z'} Pseudo logagg collect commands: $ sudo logagg collect --file file=logfile.log:myformatters.formatters.sample_formatter --nsqtopic logagg --nsqd-http-address localhost:4151 Debugging If there are multiple files being tracked by multiple collectors on multiple nodes, the collector information can be seen in \"Heartbeat\" topic of NSQ. Every running collector sends a hearbeat to this topic (default interval = 30 seconds). The heartbeat format is as follows: timestamp : Timestamp of the recieved heartbeat. heartbeat_number : The heartbeat number since the collector started running. host : Hostname of the node on which the collector is running. nsq_topic : The nsq topic which the collector is using. * files_tracked : list of files that are being tracked by the collector followed by the fomatter. You can run the following command to see the information: $ nsq_tail --topic=Heartbeat --channel=test --lookupd-http-address=<nsq-server-ip-or-DNS>:4161 Build on logagg You're more than welcome to hack on this:-) $ git clone https://github.com/deep-compute/logagg-collector $ cd logagg $ sudo python setup.py install $ docker build -t logagg .","title":"logagg-collector"},{"location":"collector/#logagg-collector","text":"Log collector for logagg Track and collects all the logs from given files and parses them to make a common schema for all the logs and sends to NSQ.","title":"logagg-collector"},{"location":"collector/#prerequisites","text":"Python >= 3.5 We expect users to follow Best practices for logging their application. Most importantly, do structured logging. Since, parsing/formatting logs is way easier that way. Install, set-up and run logagg-fs beforehand.","title":"Prerequisites"},{"location":"collector/#componentsarchitectureterminology","text":"files : Log files which are being tracked by logagg node : The server(s) where the log files reside formatters : The parser function that the collector uses to format the log lines to put it the common format. nsq : The central location where logs are sent by collector (s) after formatting as messages.","title":"Components/Architecture/Terminology"},{"location":"collector/#features","text":"Guaranteed delivery of each log line from files. Reduced latency between a log being generated an being present in the nsq . Options to add custom formatters . File poll if log file not yet generated. Works on rotational log files. Custom formatters to support parsing of any log file. Output format of processed log lines (dictionary) id (str) - A unique id per log with time ordering. Useful to avoid storing duplicates. timestamp (str) - ISO Format time. eg: data (dict) - Parsed log data raw (str) - Raw log line read from the log file host (str) - Hostname of the node where this log was generated formatter (str) - name of the formatter that processed the raw log line file (str) - Full path of the file in the host where this log line came from type (str) - One of \"log\", \"metric\" (Is there one more type?) level (str) - Log level of the log line. event (str) - LOG event error (bool) - True if collection handler failed during processing error_tb (str) - Error traceback","title":"Features"},{"location":"collector/#installation","text":"","title":"Installation"},{"location":"collector/#setup","text":"Install and run logagg-fs on the machine from where you need to collect the logs from. Install and run logagg-master service on the machine where you want to aggregate the logs.","title":"Setup"},{"location":"collector/#install-the-docker-package-at-both-collector-nodes","text":"Run the following commands to install : $ sudo apt-get update $ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - $ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" $ sudo apt-get update $ sudo apt-get install docker-ce Check Docker version >= 17.12.1 $ sudo docker -v Docker version 18.03.1-ce, build 9ee9f40 Run serverstats to store server metrics in /var/log/serverstats $ docker plugin install deepcompute/docker-file-log-driver:1.0 --alias file-log-driver $ docker run --hostname $HOSTNAME --name serverstats --restart unless-stopped --label formatter=logag_gcollector.formatters.basescript --log-driver file-log-driver --log-opt labels=formatter --log-opt fpath=/serverstats/serverstats.log --detach deepcompute/serverstats:latest","title":"Install the Docker package, at both collector nodes."},{"location":"collector/#install-the-logagg-collector-package-at-where-we-collect-the-logs","text":"Run the following command to pip install logagg : $ pip3 install https://github.com/deep-compute/pygtail/tarball/master/#egg=pygtail-0.6.1 $ pip3 install git+https://github.com/deep-compute/logagg-utils.git $ pip3 install git+https://github.com/deep-compute/logagg-collector.git","title":"Install the logagg-collector package, at where we collect the logs:"},{"location":"collector/#basic-usage","text":"","title":"Basic Usage"},{"location":"collector/#run-logagg-collector-service","text":"$ sudo logagg-collector runserver --host <DNS/IP> --port <port_number> --master host=<master_host>:port=<master_port>:topic_name=<topic_name> --data-dir <path_to_data_dir> --logaggfs-dir <logagg-fs_logcache_directory> Command breakdown: --host <DNS/IP>: DNS/IP on which collector should run so that it is accesible to other components via API calls. --port <port_number>: Port number to run logagg-collector on. --master host=<master_host>:port=<master_port>:topic_name=<topic_name>: Master host, port to contact master and topic name in which messages are sent. To aggregate logs in one place, Run all collectors under same topic name. --data-dir <path_to_data_dir>: Directory where logagg-collector will store it's data. If running multiple collectors, specify different directories. --logaggfs-dir <logagg-fs_logcache_directory>: logagg-fs logcache directory path.","title":"Run logagg-collector service"},{"location":"collector/#apis","text":"To see files collector is supposed to collect curl 'http://localhost:1099/collector/v1/get_files' To add files so that collector can poll/track them curl 'http://localhost:1099/collector/v1/add_file?fpath=\"/var/log/serverstats.log\"&formatter=\"logagg_collector.formatters.docker_file_log_driver\"' Full list of APIs here","title":"APIs"},{"location":"collector/#types-of-handlers-we-support","text":"Formatter-name Comments logagg_collector.formatters.nginx_access See Configuration here logagg_collector.formatters.mongodb logagg_collector.formatters.basescript logagg_collector.formatters.docker_log_file_driver See example here","title":"Types of handlers we support"},{"location":"collector/#advanced-usage","text":"","title":"Advanced usage"},{"location":"collector/#run-collector-without-a-master","text":"","title":"Run collector without a master"},{"location":"collector/#install-nsq-package-to-pool-logs-sent-by-collectors","text":"$ sudo apt-get install libsnappy-dev $ wget https://s3.amazonaws.com/bitly-downloads/nsq/nsq-1.0.0-compat.linux-amd64.go1.8.tar.gz $ tar zxvf nsq-1.0.0-compat.linux-amd64.go1.8.tar.gz $ sudo cp nsq-1.0.0-compat.linux-amd64.go1.8/bin/* /usr/local/bin","title":"Install NSQ package to pool logs sent by collectors"},{"location":"collector/#bring-up-the-nsq-instances-at-the-required-server-with-following-commands","text":"NOTE: Run each command in a seperate Terminal window nsqlookupd $ nsqlookupd nsqd -lookupd-tcp-address :4160 $ nsqd -lookupd-tcp-address localhost:4160 nsqadmin -lookupd-http-address :4161 $ nsqadmin -lookupd-http-address localhost:4161","title":"Bring up the nsq instances at the required server with following commands:"},{"location":"collector/#run-logagg-collector-service_1","text":"$ logagg-collector runserver --data-dir <path_to_data_dir> --logaggfs-dir <logagg-fs_logcache_directory>","title":"Run logagg-collector service"},{"location":"collector/#add-nsq-details-manually-via-api-command","text":"$ curl 'http://localhost:1099/logagg/v1/set_nsq?nsqd_http_address=localhost:4151&topic_name=logagg'","title":"Add NSQ details manually via API command."},{"location":"collector/#start-collecting","text":"$ curl 'http://localhost:1099/logagg/v1/start","title":"Start collecting"},{"location":"collector/#how-to-create-and-use-custom-formatters-for-log-files","text":"","title":"How to create and use custom formatters for log files"},{"location":"collector/#step-1-make-a-directory-and-append-its-path-to-evironment-variable-pythonpath","text":"$ echo $PYTHONPATH $ mkdir customformatters $ #Now append the path to $PYTHONPATH $ export PYTHONPATH=$PYTHONPATH:/home/path/to/customformatters/ $ echo $PYTHONPATH :/home/path/to/customformatters","title":"Step 1: make a directory and append it's path to evironment variable $PYTHONPATH"},{"location":"collector/#step-2-create-a-another-directory-and-put-your-formatter-files-inside-it","text":"$ cd customformatters/ $ mkdir myformatters $ cd myformatters/ $ touch formatters.py $ touch __init__.py $ echo 'import formatters' >> __init__.py $ #Now write your formatter functions inside the formatters.py file","title":"Step 2: Create a another directory and put your formatter file(s) inside it."},{"location":"collector/#step-3-write-your-formatter-functions-inside-the-formatterspy-file","text":"Important: 1. Only python standard modules can be imported in formatters.py file 2. A formatter function should return a dict() datatype 3. The 'dict()' should only contain keys which are mentioned in the above log structure . 4. Sample formatter functions: import json import re sample_log_line = '2018-02-07T06:37:00.297610Z [Some_event] [Info] [Hello_there]' def sample_formatter(log_line): log = re.sub('[\\[+\\]]', '',log_line).split(' ') timestamp = log[0] event = log[1] level = log[2] data = dict({'message': log[3]}) return dict(timestamp = timestamp, event = event, level = level, data = data, ) To see more examples, look here Check if the custom handler works in python interpreter . >>> import myformatters >>> sample_log_line = '2018-02-07T06:37:00.297610Z [Some_event] [Info] [Hello_there]' >>> output = myformatters.formatters.sample_formatter(sample_log_line) >>> from pprint import pprint >>> pprint(output) {'data': {'message': 'Hello_there'}, 'event': 'Some_event', 'level': 'Info', 'timestamp': '2018-02-07T06:37:00.297610Z'} Pseudo logagg collect commands: $ sudo logagg collect --file file=logfile.log:myformatters.formatters.sample_formatter --nsqtopic logagg --nsqd-http-address localhost:4151","title":"Step 3: Write your formatter functions inside the formatters.py file"},{"location":"collector/#debugging","text":"If there are multiple files being tracked by multiple collectors on multiple nodes, the collector information can be seen in \"Heartbeat\" topic of NSQ. Every running collector sends a hearbeat to this topic (default interval = 30 seconds). The heartbeat format is as follows: timestamp : Timestamp of the recieved heartbeat. heartbeat_number : The heartbeat number since the collector started running. host : Hostname of the node on which the collector is running. nsq_topic : The nsq topic which the collector is using. * files_tracked : list of files that are being tracked by the collector followed by the fomatter. You can run the following command to see the information: $ nsq_tail --topic=Heartbeat --channel=test --lookupd-http-address=<nsq-server-ip-or-DNS>:4161","title":"Debugging"},{"location":"collector/#build-on-logagg","text":"You're more than welcome to hack on this:-) $ git clone https://github.com/deep-compute/logagg-collector $ cd logagg $ sudo python setup.py install $ docker build -t logagg .","title":"Build on logagg"},{"location":"fs/","text":"logagg-fs Fuse file system for logagg-collector . Captures logs when it is written to a file and caches them until the logagg-collector collects and processes the contents. Features Guarantees capturing every log line Rotation proof One time set-up Supports file patterns; i.e. /var/log/syslog* ; rather than fpaths Limitations No way of getting logs from files before start-up of the program Requires a reboot of the machine after set-up is done Components/Architecture/Terminology mountpoint : path to the directory where logs are being written (e.g. /var/log). logcache : path to the directory where the logagg-fs program stores all of it's data. logcache/mirror : directory inside logcache path which is mounted to the mountpoint directory path. If logcache path is '/logcache' and the mountpoint is '/var/log', then the directory '/logcache/mirror' is mounted on to '/var/log'. logcache/trackfiles.txt : file inside logcache directory where file-patterns are mentioned that need to be tracked by logagg-fs (eg. '/var/log/syslog') logcache/logs : path to directory where log-files that are cached temprorarily until processed and deleted. Prerequisites Python => 3.6 Expected restart of server after mounting to non-empty directories like /var/log/ Installation Dependencies $ sudo apt install libfuse-dev python3-dev python3-pip pkg-config build-essential python3-pip $ pip3 install setuptools Install logagg-fs NOTE: Make sure you are a root user. Root user $ pip3 install git+https://github.com/deepcompute/logagg-collector.git Check installation $ logagg-fs --version logagg-fs 0.3.1 logagg-fs 0.3.1 Setting up logagg-fs for mounting /logcache/mirror to /var/log Make a directory so that logagg-fs can use it as logcache # mkdir /logcache/ Write configuration to mount /logcache/mirror to /var/log/ directory in fstab # vim /etc/fstab # Add the following line to /etc/fstab: \"logagg-fs /var/log/ fuse rw,user,auto,exec,nonempty,allow_other,root=/logcache/,loglevel=INFO,logfile=/logcache/fuse.log 0 0\" Command breakdown: logagg-fs : the path to logagg-fs program /var/log/ : the mountpoint root=/logcache/ : the logcache directory for logagg-fs logfile=/logcache/fuse.log : path where logagg-fs is supposed to store own logs Setting up logrotate for the log file of logagg-fs (Optional) Create configuration file of logrotate $ vim /etc/logrotate.d/logagg-fs Write the following lines in the file /logcache/fuse.log { weekly rotate 3 size 10M compress delaycompress } Run & Reboot to load the configuration in /etc/fstab IMPORTANT: Copy files all inside mountpoint directory to a temprorary location. # mkdir ~/temp_logs && cp -R /var/log/* ~/temp_logs/ Mount logagg-fs from fstab configuration # mount /var/log/ Copy back files to mountpoint directory # cp -R ~/temp_logs/log/* /var/log/ Reboot to make changes to take effect and running programs to use the mountpoint as storage location for logs # reboot Usage Check if '/logcache/mirror' is mounted properly to '/var/log' # ls /var/log/ # # The same as: # ls /logcache/mirror/ # cat /logcache/mirror/test # cat: /logcache/mirror/test: No such file or directory # echo \"testing..\" > /var/log/test # cat /logcache/mirror/test testing.. Check caching of log files # ls /logcache/logs/ # No logs yet # # Now add the files to be tracked in logcache/trackfiles.txt file # echo \"/var/log/syslog\" >> /logcache/trackfiles.txt # # Takes atmost 10sec to update state # ls /logcache/logs/ # To see the cached log-files f5fdf6ea0ea92860c6a6b2b354bfcbbc.1536590719.4519932.log # tail -f /logcache/logs/* # The contents of the file are being written simultaneously to cached files Just remove the file pattern from /logcache/trackfiles.txt to stop caching of logs To unmount directory # umount /var/log Or Delete configuration from /etc/fstab # reboot","title":"logagg-fs"},{"location":"fs/#logagg-fs","text":"Fuse file system for logagg-collector . Captures logs when it is written to a file and caches them until the logagg-collector collects and processes the contents.","title":"logagg-fs"},{"location":"fs/#features","text":"Guarantees capturing every log line Rotation proof One time set-up Supports file patterns; i.e. /var/log/syslog* ; rather than fpaths","title":"Features"},{"location":"fs/#limitations","text":"No way of getting logs from files before start-up of the program Requires a reboot of the machine after set-up is done","title":"Limitations"},{"location":"fs/#componentsarchitectureterminology","text":"mountpoint : path to the directory where logs are being written (e.g. /var/log). logcache : path to the directory where the logagg-fs program stores all of it's data. logcache/mirror : directory inside logcache path which is mounted to the mountpoint directory path. If logcache path is '/logcache' and the mountpoint is '/var/log', then the directory '/logcache/mirror' is mounted on to '/var/log'. logcache/trackfiles.txt : file inside logcache directory where file-patterns are mentioned that need to be tracked by logagg-fs (eg. '/var/log/syslog') logcache/logs : path to directory where log-files that are cached temprorarily until processed and deleted.","title":"Components/Architecture/Terminology"},{"location":"fs/#prerequisites","text":"Python => 3.6 Expected restart of server after mounting to non-empty directories like /var/log/","title":"Prerequisites"},{"location":"fs/#installation","text":"","title":"Installation"},{"location":"fs/#dependencies","text":"$ sudo apt install libfuse-dev python3-dev python3-pip pkg-config build-essential python3-pip $ pip3 install setuptools","title":"Dependencies"},{"location":"fs/#install-logagg-fs","text":"NOTE: Make sure you are a root user.","title":"Install logagg-fs"},{"location":"fs/#root-user","text":"$ pip3 install git+https://github.com/deepcompute/logagg-collector.git","title":"Root user"},{"location":"fs/#check-installation","text":"$ logagg-fs --version logagg-fs 0.3.1 logagg-fs 0.3.1","title":"Check installation"},{"location":"fs/#setting-up-logagg-fs-for-mounting-logcachemirror-to-varlog","text":"","title":"Setting up  logagg-fs for mounting /logcache/mirror to /var/log"},{"location":"fs/#make-a-directory-so-that-logagg-fs-can-use-it-as-logcache","text":"# mkdir /logcache/","title":"Make a directory so that logagg-fs can use it as logcache"},{"location":"fs/#write-configuration-to-mount-logcachemirror-to-varlog-directory-in-fstab","text":"# vim /etc/fstab # Add the following line to /etc/fstab: \"logagg-fs /var/log/ fuse rw,user,auto,exec,nonempty,allow_other,root=/logcache/,loglevel=INFO,logfile=/logcache/fuse.log 0 0\"","title":"Write configuration to mount /logcache/mirror to /var/log/ directory in fstab"},{"location":"fs/#command-breakdown","text":"logagg-fs : the path to logagg-fs program /var/log/ : the mountpoint root=/logcache/ : the logcache directory for logagg-fs logfile=/logcache/fuse.log : path where logagg-fs is supposed to store own logs","title":"Command breakdown:"},{"location":"fs/#setting-up-logrotate-for-the-log-file-of-logagg-fs-optional","text":"Create configuration file of logrotate $ vim /etc/logrotate.d/logagg-fs Write the following lines in the file /logcache/fuse.log { weekly rotate 3 size 10M compress delaycompress }","title":"Setting up logrotate for the log file of logagg-fs (Optional)"},{"location":"fs/#run-reboot-to-load-the-configuration-in-etcfstab","text":"IMPORTANT: Copy files all inside mountpoint directory to a temprorary location. # mkdir ~/temp_logs && cp -R /var/log/* ~/temp_logs/ Mount logagg-fs from fstab configuration # mount /var/log/ Copy back files to mountpoint directory # cp -R ~/temp_logs/log/* /var/log/ Reboot to make changes to take effect and running programs to use the mountpoint as storage location for logs # reboot","title":"Run &amp; Reboot to load the configuration in /etc/fstab"},{"location":"fs/#usage","text":"Check if '/logcache/mirror' is mounted properly to '/var/log' # ls /var/log/ # # The same as: # ls /logcache/mirror/ # cat /logcache/mirror/test # cat: /logcache/mirror/test: No such file or directory # echo \"testing..\" > /var/log/test # cat /logcache/mirror/test testing.. Check caching of log files # ls /logcache/logs/ # No logs yet # # Now add the files to be tracked in logcache/trackfiles.txt file # echo \"/var/log/syslog\" >> /logcache/trackfiles.txt # # Takes atmost 10sec to update state # ls /logcache/logs/ # To see the cached log-files f5fdf6ea0ea92860c6a6b2b354bfcbbc.1536590719.4519932.log # tail -f /logcache/logs/* # The contents of the file are being written simultaneously to cached files Just remove the file pattern from /logcache/trackfiles.txt to stop caching of logs To unmount directory # umount /var/log Or Delete configuration from /etc/fstab # reboot","title":"Usage"}]}